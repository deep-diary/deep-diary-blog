(window.webpackJsonp=window.webpackJsonp||[]).push([[104],{727:function(t,s,a){"use strict";a.r(s);var n=a(5),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("说明")]),t._v(" "),a("p",[t._v("​\t本文主要介绍OpenAI api 的实用，后续可以结合语音识别和播放功能，为后续聊天机器人做准备。")]),t._v(" "),a("p",[a("a",{attrs:{href:"www.deep-diary.com"}},[t._v("点击免费观看教学视频")])])]),t._v(" "),a("h1",{attrs:{id:"text-openai"}},[t._v("Text OpenAI")]),t._v(" "),a("h2",{attrs:{id:"安装对应的包"}},[t._v("安装对应的包")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("pip install "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("upgrade openai\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h2",{attrs:{id:"简单使用"}},[t._v("简单使用")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" openai\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" config "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OPENAI_API_KEY\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TextOpenAi")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 设置 API Key，申请地址：https://platform.openai.com/account/api-keys")]),t._v("\n        openai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("api_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" OPENAI_API_KEY\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 设置组织，查看地址：https://platform.openai.com/account/org-settings")]),t._v("\n        openai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("organization "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org-MmExD0t6JKxXSrxJCLhXdTVd"')]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 请求模型")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_chat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpt-3.5-turbo"')]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text-davinci-003"')]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conversation "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Blue'")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bot_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'DiaryAI'")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# list models")]),t._v("\n        models "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" openai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print the first model's id")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("models"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("id")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("on_conversation")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        prompt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('": "')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" text "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\n"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bot_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('": "')]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("prompt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conversation "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" prompt\n\n        response "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" openai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Completion"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            model"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            prompt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conversation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            max_tokens"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            temperature"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            top_p"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            frequency_penalty"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            presence_penalty"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        response_str "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        response_str "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response_str"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('":"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bot_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('":"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conversation "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" response_str "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\n"')]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response_str"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conversation\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# completion = openai.ChatCompletion.create(")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     model=model,")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     messages=[")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#         {"role": "system", "content": "You are a helpful assistant."},')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#         {"role": "user", "content": "Who won the world series in 2020?"},')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#         {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#         {"role": "user", "content": "Where was it played?"}')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     ]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# )")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(completion.choices[0].message)")]),t._v("\n\ntext_ai "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TextOpenAi"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    text "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("input")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('">>"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    text_ai"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("on_conversation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br"),a("span",{staticClass:"line-number"},[t._v("33")]),a("br"),a("span",{staticClass:"line-number"},[t._v("34")]),a("br"),a("span",{staticClass:"line-number"},[t._v("35")]),a("br"),a("span",{staticClass:"line-number"},[t._v("36")]),a("br"),a("span",{staticClass:"line-number"},[t._v("37")]),a("br"),a("span",{staticClass:"line-number"},[t._v("38")]),a("br"),a("span",{staticClass:"line-number"},[t._v("39")]),a("br"),a("span",{staticClass:"line-number"},[t._v("40")]),a("br"),a("span",{staticClass:"line-number"},[t._v("41")]),a("br"),a("span",{staticClass:"line-number"},[t._v("42")]),a("br"),a("span",{staticClass:"line-number"},[t._v("43")]),a("br"),a("span",{staticClass:"line-number"},[t._v("44")]),a("br"),a("span",{staticClass:"line-number"},[t._v("45")]),a("br"),a("span",{staticClass:"line-number"},[t._v("46")]),a("br"),a("span",{staticClass:"line-number"},[t._v("47")]),a("br"),a("span",{staticClass:"line-number"},[t._v("48")]),a("br"),a("span",{staticClass:"line-number"},[t._v("49")]),a("br"),a("span",{staticClass:"line-number"},[t._v("50")]),a("br"),a("span",{staticClass:"line-number"},[t._v("51")]),a("br"),a("span",{staticClass:"line-number"},[t._v("52")]),a("br"),a("span",{staticClass:"line-number"},[t._v("53")]),a("br"),a("span",{staticClass:"line-number"},[t._v("54")]),a("br"),a("span",{staticClass:"line-number"},[t._v("55")]),a("br"),a("span",{staticClass:"line-number"},[t._v("56")]),a("br"),a("span",{staticClass:"line-number"},[t._v("57")]),a("br"),a("span",{staticClass:"line-number"},[t._v("58")]),a("br"),a("span",{staticClass:"line-number"},[t._v("59")]),a("br"),a("span",{staticClass:"line-number"},[t._v("60")]),a("br"),a("span",{staticClass:"line-number"},[t._v("61")]),a("br"),a("span",{staticClass:"line-number"},[t._v("62")]),a("br")])]),a("p",[t._v("效果如下：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://deep-diary.oss-cn-hangzhou.aliyuncs.com/blog/b4c3048a74955a2d3a28b4300f7f637.png",alt:"b4c3048a74955a2d3a28b4300f7f637"}})]),t._v(" "),a("p",[t._v("这里使用的是通过命令行文字输入，但尴尬的是把输入的内容再次呈现出来了，实际上可以更改下"),a("code",[t._v("input")]),t._v("函数，同时，可以不用打印刚输入的内容。")]),t._v(" "),a("p",[t._v("不过值得注意的是，后续切换成语音模式的时候，还是需要把语音识别的结果打印出来的.")]),t._v(" "),a("p",[t._v("一顿整改过后，效果好多了，先跟他玩一会吧：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://deep-diary.oss-cn-hangzhou.aliyuncs.com/blog/image-20230603110750762.png",alt:"image-20230603110750762"}})]),t._v(" "),a("p",[t._v("全部的聊天内容如下")]),t._v(" "),a("div",{staticClass:"language-bash line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: my name is blue, and my wife is susan. we had a baby, could you "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("help")]),t._v(" to give us a child name with means\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: How about the name Diara, "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("which")]),t._v(" means "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"precious gift"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" Hebrew?\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: sounds cool, we also prepared anther name, call allison. "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("do")]),t._v(" you know some meaning "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" this name?\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: Allison is derived from the Germanic name "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Alis"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("which")]),t._v(" means "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"noble"')]),t._v(" or "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"kind"')]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(".")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: cool, once I heard from the movie, that "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" the name start from A, the people will be "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("more")]),t._v(" cliever since "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" their name start with A, they will called on by the teacher often. how "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("do")]),t._v(" you think about this?\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: While there is no scientific evidence to support the idea that people whose names start with A are "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("more")]),t._v(" intelligent, it is "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),t._v(" that people whose names start with A are "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("more")]),t._v(" likely to be called upon by teachers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" the classroom. This is due to the fact that names that begin with A are typically at the "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("top")]),t._v(" of alphabetically sorted lists.\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: thanks "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" you answer, it is my pleaure to talk with you. now I have another question?\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: Sure, what would you like to know?\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: I keen to record every moments that happen during our daily life. the recording mathoned such like take a picture, writing a diary with gps info, the chat infomation from apps like wechat, audio information etc. after I parepared enough data, I want to train all of them by using GCN, as a result, this model includes all my memories.do you think is it possible? \n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: It is certainly possible to use GCN to train all of the data that you have collected. However, it is important to note that the accuracy of the model will depend on the quality and quantity of the data that you have collected. Additionally, it may be difficult to capture the nuances of your memories using a machine learning model.\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: ddo you familiar with digital immortality?\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: Yes, I am familiar with the concept of digital immortality. Digital immortality is the idea that an individual's memories, experiences, and personality could be preserved digitally, allowing them to exist beyond their physical death.\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: so how many stacks that needed to realized the digital immortality. \n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: In order to achieve digital immortality, a number of different technologies would need to be incorporated. These include artificial intelligence, natural language processing, machine learning, virtual reality, augmented reality, and more. Additionally, the data that is collected would need to be stored securely and safely "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" order to ensure that it is preserved "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" future generations.\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: thanks, could you give me a research report, "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("which")]),t._v(" includes "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("which")]),t._v(" company is experting "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" this field and what are the latest achievements, results "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("80")]),t._v(" words are enough, thanks\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("DiaryAI: There are several companies that are currently researching digital immortality. For example, Google is working on an AI-powered virtual assistant that could "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("help")]),t._v(" preserve memories. IBM is also researching digital immortality, focusing on using AI and natural language processing to capture memories. Additionally, Microsoft is researching ways to use virtual reality to capture memories. The latest achievements "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" this field include using AI to create virtual avatars that can interact with people "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" the real world.\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v("Blue: \n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br")])]),a("h2",{attrs:{id:"completion-和-chatcompletion-的区别"}},[t._v("'Completion '和'ChatCompletion'的区别")]),t._v(" "),a("h2",{attrs:{id:"更新记录"}},[t._v("更新记录")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",[t._v("Date")]),t._v(" "),a("th",[t._v("Updated Info")]),t._v(" "),a("th",[t._v("Address")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("2023-06-03")]),t._v(" "),a("td",[t._v("init, realized normal chat with chatgpt by text, next step is voice communication")]),t._v(" "),a("td",[t._v("@ home")])]),t._v(" "),a("tr",[a("td"),t._v(" "),a("td"),t._v(" "),a("td")]),t._v(" "),a("tr",[a("td"),t._v(" "),a("td"),t._v(" "),a("td")]),t._v(" "),a("tr",[a("td"),t._v(" "),a("td"),t._v(" "),a("td")]),t._v(" "),a("tr",[a("td"),t._v(" "),a("td"),t._v(" "),a("td")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);